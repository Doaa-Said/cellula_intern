# -*- coding: utf-8 -*-
"""Week4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/169EkL_n2i4gpbuuflQO119xUhMJuT5t4
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "Salesforce/codegen-350M-mono"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

def generate_text(prompt, max_length=2000, num_return_sequences=1):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7,
    )
    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs][0]

from langchain.llms.base import LLM
from typing import Any

class CustomHFLLM(LLM):
    def _call(self, prompt: str, stop: Any = None) -> str:
        return generate_text(prompt, max_length=500)

    @property
    def _llm_type(self) -> str:
        return "custom_huggingface"

llm = CustomHFLLM()

from datasets import load_dataset
from langchain.schema import Document  # import Document
dataset = load_dataset("openai/openai_humaneval", split="test")

# convert to list of dicts
dataset = list(dataset)
documents = []

for item in dataset:
    doc_text = f"Problem:\n{item['prompt']}\n\nSolution:\n{item.get('canonical_solution', 'Not available')}"
    metadata = {"task_id": item.get("task_id", "unknown")}
    documents.append(Document(page_content=doc_text, metadata=metadata))

"""Task0"""

from langchain_core.prompts import ChatPromptTemplate


prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are a precise code comparison assistant. "
     "Given a user's request and a retrieved code snippet from the HumanEval dataset, "
     "determine if they achieve the same purpose. "
     "If they match, return only the Python function from the retrieved code. "
     "If they differ, ignore the retrieved code and generate a new Python function that fulfills the user's request. "
     "Always return only the function, with no explanations or extra text."),
    ("user",
     "User request: {user_request}\n\nRetrieved code snippet:\n{retrieved_code}")
])

chain = prompt | llm

"""Task1"""

from langchain.memory import ChatMessageHistory
from langchain_core.runnables import RunnableWithMessageHistory


sessions = {}

def get_history(session_id: str):
    """Fetch or create chat history for a session."""
    if session_id not in sessions:
        sessions[session_id] = ChatMessageHistory()
    return sessions[session_id]

chat_with_memory = RunnableWithMessageHistory(
    chain,
    get_history,
    input_messages_key="user_request",
    history_messages_key="history"
)


def chat(user_request: str, retrieved_code: str = "", session_id: str = "default"):
    """Send request to LLM chain with session memory."""
    config = {"configurable": {"session_id": session_id}}
    response = chat_with_memory.invoke(
        {"user_request": user_request, "retrieved_code": retrieved_code},
        config=config,
        callbacks=[]
    )
    return response


output = chat(
    "I want a function that returns 'Hello'",
    retrieved_code="def greet(): print('Hello!')",
    session_id="session1"
)
print("LLM output:\n", output)

history = get_history("session1")
print("\nSession memory messages:")
for i, msg in enumerate(history.messages):
    print(f"{i+1}. {msg.type}: {msg.content}")

"""Task 2"""

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
persist_directory = 'docs/chroma/'
embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",

    )
from langchain_chroma import Chroma

vector_store = Chroma(
    collection_name="example_collection",
    embedding_function=embeddings,
    persist_directory="./chroma_langchain_db",
)

vector_store.add_documents(documents, ids=[str(i) for i in range(len(documents))])

query = "Write a function that calculate the factorial of mumber"
results = vector_store.similarity_search(query, k=1)
top_doc = results[0]
print("Page content:\n", top_doc.page_content)
print("\nMetadata:\n", top_doc.metadata)

def ask(user_request, session_id="session-zero"):

    similar_docs =vector_store.similarity_search(user_request, k=1)
    retrieved_code = similar_docs[0].page_content if similar_docs else ""


    response = chat(user_request, retrieved_code=retrieved_code, session_id=session_id)

    return response


user_input = "Write a Python function that checks if a number is prime"
generated_code = ask(user_input)

print("=== Generated / Retrieved Code ===")
print(generated_code)

"""Task 3"""

def calculate_precision_at_k(retrieved_docs, relevant_docs, k):
    """Calculate Precision@K - proportion of retrieved docs that are relevant"""
    if k == 0 or not retrieved_docs:
        return 0.0

    retrieved_k = retrieved_docs[:k]
    relevant_ids = {doc.metadata.get('task_id', '') for doc in relevant_docs}

    relevant_count = 0
    for doc in retrieved_k:
        if doc.metadata.get('task_id', '') in relevant_ids:
            relevant_count += 1

    return relevant_count / k

def calculate_recall_at_k(retrieved_docs, relevant_docs, k):
    """Calculate Recall@K - proportion of relevant docs that are retrieved"""
    if not relevant_docs:
        return 0.0

    retrieved_k = retrieved_docs[:k]
    relevant_ids = {doc.metadata.get('task_id', '') for doc in relevant_docs}
    retrieved_ids = {doc.metadata.get('task_id', '') for doc in retrieved_k}

    relevant_retrieved = len(relevant_ids.intersection(retrieved_ids))
    return relevant_retrieved / len(relevant_ids)

def calculate_mrr(retrieved_docs, relevant_docs):
    """Calculate Mean Reciprocal Rank - reciprocal of rank of first relevant doc"""
    if not relevant_docs:
        return 0.0

    relevant_ids = {doc.metadata.get('task_id', '') for doc in relevant_docs}

    for rank, doc in enumerate(retrieved_docs, 1):
        if doc.metadata.get('task_id', '') in relevant_ids:
            return 1.0 / rank

    return 0.0

def calculate_ndcg_at_k(retrieved_docs, relevant_docs, k):
    """Calculate nDCG@K - measures ranking quality"""
    if k == 0:
        return 0.0

    # Binary relevance
    relevance_scores = []
    relevant_ids = {doc.metadata.get('task_id', '') for doc in relevant_docs}

    for doc in retrieved_docs[:k]:
        if doc.metadata.get('task_id', '') in relevant_ids:
            relevance_scores.append(1)
        else:
            relevance_scores.append(0)

    # Calculate DCG
    dcg = 0.0
    for i, rel in enumerate(relevance_scores):
        dcg += rel / np.log2(i + 2)

    # Calculate ideal DCG
    ideal_relevance = [1] * min(len(relevant_docs), k)
    ideal_dcg = 0.0
    for i, rel in enumerate(ideal_relevance):
        ideal_dcg += rel / np.log2(i + 2)

    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0

def evaluate_retrieval(query, ground_truth_docs, k_values=[1, 3, 5]):
    """Comprehensive retrieval evaluation for a query"""
    retrieved_docs = vector_store.similarity_search(query, k=max(k_values))

    results = {}
    for k in k_values:
        results[f'precision@{k}'] = calculate_precision_at_k(retrieved_docs, ground_truth_docs, k)
        results[f'recall@{k}'] = calculate_recall_at_k(retrieved_docs, ground_truth_docs, k)
        results[f'ndcg@{k}'] = calculate_ndcg_at_k(retrieved_docs, ground_truth_docs, k)

    results['mrr'] = calculate_mrr(retrieved_docs, ground_truth_docs)

    return results, retrieved_docs

"""Task4"""

def evaluate_with_deepeval(query, generated_answer, retrieved_docs, ground_truth=None):
    """Single function for DeepEval evaluation - Task 4"""
    try:
        from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric, ContextualRecallMetric
        from deepeval.test_case import LLMTestCase

        # Convert documents to context
        context = [doc.page_content for doc in retrieved_docs]

        # Create test case
        test_case = LLMTestCase(
            input=query,
            actual_output=generated_answer,
            expected_output=ground_truth or generated_answer,
            context=context,
            retrieval_context=context
        )

        # Initialize and compute metrics
        answer_relevancy = AnswerRelevancyMetric(threshold=0.7)
        contextual_precision = ContextualPrecisionMetric(threshold=0.7)
        contextual_recall = ContextualRecallMetric(threshold=0.7)

        answer_relevancy.measure(test_case)
        contextual_precision.measure(test_case)
        contextual_recall.measure(test_case)

        return {
            'answer_relevancy': answer_relevancy.score,
            'contextual_precision': contextual_precision.score,
            'contextual_recall': contextual_recall.score,
            'all_passed': (answer_relevancy.score >= 0.7 and
                          contextual_precision.score >= 0.7 and
                          contextual_recall.score >= 0.7)
        }

    except ImportError:
        print("DeepEval not installed. Install with: pip install deepeval")
        return {}

# creating some example documents to test the functions
from langchain.schema import Document

# Use real data from your vector store
print("=== USING REAL VECTOR STORE DATA ===")

# Test with actual queries against your vector store
test_queries = [
    "Write a Python function that checks if a number is prime",
    "Create a function to calculate factorial",
    "Write a function to reverse a string",
    "Write a function to find the maximum number in a list"
]

ground_truth_docs = documents[:3]

print(f"Using {len(ground_truth_docs)} documents as ground truth")
print(f"Vector store has {len(documents)} total documents")

for i, query in enumerate(test_queries, 1):
    print(f"\n{'='*60}")
    print(f"QUERY {i}: {query}")
    print(f"{'='*60}")

    # Get real retrieved documents from your vector store
    retrieved_docs = vector_store.similarity_search(query, k=5)
    print(f"Retrieved {len(retrieved_docs)} documents")

    # Calculate Task 3 metrics with real data
    precision_3 = calculate_precision_at_k(retrieved_docs, ground_truth_docs, 3)
    recall_3 = calculate_recall_at_k(retrieved_docs, ground_truth_docs, 3)
    mrr_score = calculate_mrr(retrieved_docs, ground_truth_docs)
    ndcg_3 = calculate_ndcg_at_k(retrieved_docs, ground_truth_docs, 3)

    print("TASK 3 - Retrieval Metrics:")
    print(f"  Precision@3: {precision_3:.3f}")
    print(f"  Recall@3: {recall_3:.3f}")
    print(f"  MRR: {mrr_score:.3f}")
    print(f"  nDCG@3: {ndcg_3:.3f}")

    # Show what was actually retrieved
    print("\nTop 3 retrieved documents:")
    for j, doc in enumerate(retrieved_docs[:3], 1):
        print(f"  {j}. {doc.page_content[:100]}...")
        print(f"     Metadata: {doc.metadata}")

    # Test Task 4 with real generated answer
    print("\nTASK 4 - Generation Quality:")

    # Get real generated answer from your RAG system
    generated_answer = ask(query, session_id=f"test_{i}")

    print("Generated code:")
    print(generated_answer[:200] + "..." if len(generated_answer) > 200 else generated_answer)

    # Try DeepEval evaluation
    try:
        deepeval_results = evaluate_with_deepeval(query, generated_answer, retrieved_docs[:3])
        if deepeval_results:
            print("DeepEval Metrics:")
            for metric, value in deepeval_results.items():
                if metric != 'all_passed':
                    print(f"  {metric}: {value:.3f}")
            print(f"  All tests passed: {deepeval_results['all_passed']}")
        else:
            print("  DeepEval: Not available (install and set API key)")
    except Exception as e:
        print(f"  DeepEval Error: {str(e)[:100]}...")

print("\n" + "="*60)
print("REAL DATA EVALUATION COMPLETED!")
print("="*60)

# Also test the comprehensive evaluation function with real data
print("\n=== COMPREHENSIVE EVALUATION WITH REAL DATA ===")
sample_query = "Write a function that calculates factorial"
print(f"Testing comprehensive evaluation with: '{sample_query}'")

comprehensive_results, real_retrieved_docs = evaluate_retrieval(sample_query, ground_truth_docs)
print("Comprehensive Results:")
for metric, value in comprehensive_results.items():
    print(f"  {metric}: {value:.3f}")

print(f"\nActually retrieved {len(real_retrieved_docs)} documents")
print("Sample of retrieved content:")
for i, doc in enumerate(real_retrieved_docs[:2], 1):
    print(f"  {i}. {doc.page_content[:150]}...")