# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ieWZzOpVZ7RPsshRq8YUPwYcECzDoqEf
"""

!pip install langgraph langchain sentence-transformers langchain_chroma langchain_community langchain_core-cpu requests

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

def generate_text(prompt, max_length=2000, num_return_sequences=1):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7,
    )
    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs][0]

from langchain.llms.base import LLM
from typing import Any

class CustomHFLLM(LLM):
    def _call(self, prompt: str, stop: Any = None) -> str:
        return generate_text(prompt, max_length=500)

    @property
    def _llm_type(self) -> str:
        return "custom_huggingface"

llm = CustomHFLLM()

from datasets import load_dataset
from langchain_core.documents import Document  # import Document
dataset = load_dataset("openai/openai_humaneval", split="test")

# convert to list of dicts
dataset = list(dataset)
documents = []

for item in dataset:
    doc_text = f"Problem:\n{item['prompt']}\n\nSolution:\n{item.get('canonical_solution', 'Not available')}"
    metadata = {"task_id": item.get("task_id", "unknown")}
    documents.append(Document(page_content=doc_text, metadata=metadata))

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
persist_directory = 'docs/chroma/'
embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",

    )
from langchain_chroma import Chroma

vector_store = Chroma(
    collection_name="example_collection",
    embedding_function=embeddings,
    persist_directory="./chroma_langchain_db",
)

vector_store.add_documents(documents, ids=[str(i) for i in range(len(documents))])

def retrieve_code(user_request: str):
    results = vector_store.similarity_search(user_request, k=1)
    return results[0].page_content if results else ""

def classify_intent(state):
    text = state['input'].lower()
    if "explain" in text or "what does" in text:
        state['intent'] = 'explain'
    else:
        state['intent'] = 'generate'
    return state

def call_llm(state):
    user_input = state["input"]
    intent = state["intent"]

    # =========================
    #  CODE GENERATION MODE
    # =========================
    if intent == "generate":
        context = retrieve_code(user_input)
        retrieved = context[0]["code"] if context else "None"

        prompt = f"""
You are a Python code generator.

User wants to generate code for:
{user_input}

Here is a relevant example to help you:
{retrieved}

Now write a clean, correct Python function.
Return ONLY Python code.
"""

    # =========================
    #  EXPLAIN MODE
    # =========================
    else:
        prompt = f"""
You are a Python tutor.
Explain this concept in simple, clear steps:
{user_input}

If the user asks 'how to write code',
show a small example and explain it.
"""

    # Call HF local LLM (returns string)
    response = llm.invoke(prompt)

    state["output"] = response
    return state

from typing import TypedDict, Optional

class MyState(TypedDict):
    input: str
    intent: Optional[str]
    output: Optional[str]
def finalize(state: dict) -> dict:
    print("\nðŸ§  Assistant:\n", state.get("output", "[No output found]"))
    return state
from langgraph.graph import StateGraph, END
from langchain_core.runnables import RunnableLambda
# Pass the schema to the StateGraph
builder = StateGraph(MyState)

# Add  nodes
builder.add_node("chat_state", RunnableLambda(classify_intent))
builder.add_node("llm_node", RunnableLambda(call_llm))
builder.add_node("final_node", RunnableLambda(finalize))

# Define transitions
builder.set_entry_point("chat_state")
builder.add_edge("chat_state", "llm_node")
builder.add_edge("llm_node", "final_node")
builder.add_edge("final_node", END)

# Compile the graph
graph = builder.compile()

user_input = input("ðŸ‘¤ You: ")
state = {"input": user_input}
graph.invoke(state)

from fastapi import FastAPI
import uvicorn

app = FastAPI(title="LangGraph LLM API")

@app.post("/run")
def run_graph(user_input: str):
    # Create initial graph state
    state = {"input": user_input, "intent": None, "output": None}

    # Run the graph
    result = graph.invoke(state)

    # Return only final output (not prints)
    return {"output": result.get("output")}

def start_server():
    uvicorn.run(app, host="0.0.0.0", port=8000)

import threading
threading.Thread(target=start_server).start()
