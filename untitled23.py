# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jVlbylC51e-YbrLQs9tO177yMawQiDQe

Task0
"""

!pip install langchain langchain-huggingface transformers

from langchain.schema import SystemMessage, HumanMessage
!pip install -U langchain langchain-openai langchain-community openai --quiet

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

from langchain.llms import HuggingFacePipeline

model_id = "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-3b"

tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)

base_model = AutoModelForCausalLM.from_pretrained(model_id,

                                             device_map="cuda" )

pipe = pipeline("text-generation",
                model=base_model,
                tokenizer=tokenizer,
                max_new_tokens=256,
                temperature=0.6,
                top_p=0.95,
                repetition_penalty=1.2)

llm = HuggingFacePipeline(pipeline=pipe)

system_prompt = (
    "You are a helpful programming assistant. "
    "Always write clean, well-commented Python code, and avoid extra conversation."
)

user_input = "Write a Python function to compute factorial recursively."

# Combine manually into one instruction
final_prompt = f"{system_prompt}\n\nUser request:\n{user_input}\n\nAnswer:\n"

# Generate
response = llm.invoke(final_prompt)
print(response)